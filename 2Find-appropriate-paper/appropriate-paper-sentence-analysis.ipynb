{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize \n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "# import torch\n",
    "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# from transformers import BertTokenizer, BertConfig\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import transformers\n",
    "# from transformers import *\n",
    "# from transformers import BertForTokenClassification, AdamW\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "# from seqeval.metrics import f1_score\n",
    "# from seqeval.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# import gensim\n",
    "# from gensim.utils import simple_preprocess\n",
    "# import gensim.corpora as corpora\n",
    "# from pprint import pprint\n",
    "\n",
    "\n",
    "# from numpy.random import seed\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import keras\n",
    "# from keras import backend as K\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "# from itertools import chain\n",
    "# from tensorflow.keras import Model,Input\n",
    "# from tensorflow.keras.layers import LSTM,Embedding,Dense\n",
    "# from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D,Bidirectional\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "# print(torch.__version__)\n",
    "# print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is blanked intentionally to write comments or other stuffs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "# for token in doc:\n",
    "#     print(token.text, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction method for any text input, will be used for each sentence after the cleaning method is applied\n",
    "## This is for deep learning model\n",
    "\n",
    "# def create_test_input_from_text(text):\n",
    "#     text = re.sub(r\"\\.\",\"\",text)\n",
    "#     text = re.sub(r\"\\/\",\"\",text)\n",
    "#     word_list = word_tokenize(text)\n",
    "# #     word_list = text.split(\" \")\n",
    "# #     word_list = pad_sequences(maxlen = max_len, sequences = word_list, padding = 'post', value = num_words - 1)\n",
    "#     x_new = []\n",
    "#     for word in word_list:\n",
    "#         try:\n",
    "# #         word = re.sub(r\"'\",\"\",word)\n",
    "#             x_new.append(word_idx[word])\n",
    "# #             x_new.append(float(word))\n",
    "#         except:\n",
    "#             print(\"\")\n",
    "        \n",
    "#     p = model.predict(np.array([x_new]))\n",
    "#     p = np.argmax(p, axis = -1)\n",
    "#     print(\"{:20}\\t{}\\n\".format(\"Word\", \"Prediction\"))\n",
    "#     print(\"-\" * 35)\n",
    "\n",
    "#     for (w, pred) in zip(range(len(x_new)), p[0]):\n",
    "#         print(\"{:20}\\t{}\".format(word_list[w], tags[pred]))\n",
    "    \n",
    "#     print(word_list,'\\n=====\\n',x_new)\n",
    "\n",
    "# test_inputs = \"Lithium bistrifluoromethane sulfonyl imide LiTFSI salt are potentially a good alternative to LiPF6 since it could both improve the chemical and thermal stability as salt for electrolyte\"\n",
    "# create_test_input_from_text(test_inputs)\n",
    "\n",
    "# test_inputs = \"Li metal foil as the counter electrode and 1 M LiPF6 solution with ethylene carbonate-dimethyl carbonate as the electrolyte\"\n",
    "# create_test_input_from_text(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## System Training with deep learning LSTM model ; to use this need to uncomment all the codes of this block\n",
    "## As metrices have been removed from Keras from version 2.0, they need to be calculated manually.\n",
    "## Reference : https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def recall_m(y_true, y_pred):\n",
    "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "#     recall = true_positives / (possible_positives + K.epsilon())\n",
    "#     return recall\n",
    "\n",
    "# def precision_m(y_true, y_pred):\n",
    "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
    "#     return precision\n",
    "\n",
    "# def f1_m(y_true, y_pred):\n",
    "#     precision = precision_m(y_true, y_pred)\n",
    "#     recall = recall_m(y_true, y_pred)\n",
    "#     return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# # getting the data\n",
    "# file = \"EDLC_ner_dataset_V3.csv\"\n",
    "# data = pd.read_csv(file, encoding = \"utf-8\")\n",
    "\n",
    "# # drop the docId column, it is not needed\n",
    "# data = data.drop('docId', 1)\n",
    "\n",
    "\n",
    "# # print(data.head())\n",
    "\n",
    "# words = list(set(data[\"Word\"].values))\n",
    "# words.append(\"ENDPAD\")\n",
    "# num_words = len(words)\n",
    "\n",
    "# # print(f\"Total number of unique words in dataset: {num_words}\")\n",
    "\n",
    "# tags = list(set(data[\"Tag\"].values))\n",
    "# num_tags = len(tags)\n",
    "# num_tags\n",
    "# # print(\"List of tags: \" + ', '.join([tag for tag in tags]))\n",
    "# # print(f\"Total Number of tags {num_tags}\")\n",
    "\n",
    "# class Get_sentence(object):\n",
    "#     def __init__(self,data):\n",
    "#         self.n_sent = 1\n",
    "#         self.data = data\n",
    "#         agg_func = lambda s:[(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "#                                                     s[\"Tag\"].values.tolist())]\n",
    "#         self.grouped = self.data.groupby(\"SentenceID\").apply(agg_func)\n",
    "#         self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "        \n",
    "# getter = Get_sentence(data)\n",
    "# sentence = getter.sentences\n",
    "# # print(sentence[10])\n",
    "\n",
    "# ## Exploratory Data Analysis of the loaded dataset ########\n",
    "\n",
    "# # plt.figure(figsize=(14,7))\n",
    "# # plt.hist([len(s) for s in sentence],bins = 100)\n",
    "# # plt.xlabel(\"Length of Sentences\")\n",
    "# # plt.show()\n",
    "\n",
    "# # plt.figure(figsize=(14, 7))\n",
    "# # plt.xlabel(\"Number of Tags\")\n",
    "# # data.Tag[data.Tag != 'O']\\\n",
    "# #     .value_counts()\\\n",
    "# #     .plot\\\n",
    "# #     .barh();\n",
    "\n",
    "\n",
    "# ## Exploratory Data Analysis of the loaded dataset ends! ###########\n",
    "\n",
    "# word_idx = {w : i + 1 for i ,w in enumerate(words)}\n",
    "# tag_idx =  {t : i for i ,t in enumerate(tags)}\n",
    "\n",
    "# print(\"Word idx: \",word_idx)\n",
    "# # print(tag_idx)\n",
    "\n",
    "# max_len = 60\n",
    "# X = [[word_idx[w[0]] for w in s] for s in sentence]\n",
    "# X = pad_sequences(maxlen = max_len, sequences = X, padding = 'post', value = num_words - 1)\n",
    "\n",
    "# y = [[tag_idx[w[1]] for w in s] for s in sentence]\n",
    "# y = pad_sequences(maxlen = max_len, sequences = y, padding = 'post', value = tag_idx['O'])\n",
    "# y = [to_categorical(i, num_classes = num_tags) for i in  y]\n",
    "\n",
    "# x_train,x_test,y_train,y_test = train_test_split(X, y,test_size = 0.1, random_state = 1)\n",
    "\n",
    "# input_word = Input(shape = (max_len,))\n",
    "# model = Embedding(input_dim = num_words, output_dim = max_len, input_length = max_len)(input_word)\n",
    "# model = SpatialDropout1D(0.01)(model)\n",
    "# model = Bidirectional(LSTM(units = 1000,return_sequences = True, recurrent_dropout = 0.01))(model)\n",
    "# out = TimeDistributed(Dense(num_tags,activation = 'softmax'))(model)\n",
    "# model = Model(input_word,out)\n",
    "\n",
    "\n",
    "# model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy',precision_m, recall_m, f1_m])\n",
    "\n",
    "# # model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])   ## Main compile phase\n",
    "\n",
    "# ## print model summary\n",
    "\n",
    "# # model.summary()  \n",
    "\n",
    "# # plot_model(model, show_shapes = True)\n",
    "\n",
    "# # model.fit(x_train, np.array(y_train), batch_size = 16, verbose = 1, epochs = 50, validation_split = 0.2)\n",
    "\n",
    "\n",
    "# tensorboard_cbk = TensorBoard(log_dir=\"logs/\")\n",
    "\n",
    "# history = model.fit(x_train, np.array(y_train), batch_size = 16, verbose = 1, epochs = 1, validation_split = 0.2,callbacks=[tensorboard_cbk])\n",
    "\n",
    "\n",
    "# print(\"Model Evaluation \\n===========================\\n\")\n",
    "# model.evaluate(x_test, np.array(y_test))\n",
    "\n",
    "# rand_sent = np.random.randint(0, x_test.shape[0]) # get a random sentence\n",
    "# p = model.predict(np.array([x_test[rand_sent]]))\n",
    "# p = np.argmax(p, axis = -1)\n",
    "\n",
    "# y_true = np.argmax(np.array(y_test), axis = -1)[rand_sent] # get actual tags for random sentense\n",
    "\n",
    "# print(\"{:20}{:20}\\t{}\\n\".format(\"Word\", \"True\", \"Pred\"))\n",
    "# print(\"-\" * 55)\n",
    "\n",
    "# for (w, t, pred) in zip(x_test[rand_sent], y_true, p[0]):\n",
    "#     print(\"{:20}{:20}\\t{}\".format(words[w - 1], tags[t], tags[pred]))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# ## Plotting all the metrices in a single figure\n",
    "# pd.DataFrame(history.history).plot(figsize=(16,10))\n",
    "# plt.show()\n",
    "\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ###########################################################################################################\n",
    "# #                                        Saving and loading model                                         #\n",
    "# ###########################################################################################################\n",
    "# ## Reference : https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model\n",
    "\n",
    "# ### As model can not be restored with custom metrices, hence only accuracy and loss is there and compiled the model again\n",
    "# ### Then fit the model and save the model\n",
    "\n",
    "# # model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "# # model.fit(x_train, np.array(y_train), batch_size = 16, verbose = 1, epochs = 3, validation_split = 0.2)\n",
    "\n",
    "# # model.evaluate(x_test, np.array(y_test))\n",
    "\n",
    "\n",
    "# # ## Save model\n",
    "# # model.save('DeepLearningKerasTf_NER_Model.h5')\n",
    "\n",
    "\n",
    "# ###########################################################################################################\n",
    "# #                                        Restoring a Saved model                                          #\n",
    "# ###########################################################################################################\n",
    "\n",
    "# ## restoring model and checking model\n",
    "# ## Currently model restoration feature is off. to load the model just uncomment the following lines and provide saved model name\n",
    "\n",
    "\n",
    "# # new_model = keras.models.load_model('DeepLearningKerasTf_NER_Model.h5')\n",
    "# # print(\"Loaded Model Evaluation \\n===========================\\n\")\n",
    "# # new_model.evaluate(x_test, np.array(y_test))\n",
    "\n",
    "\n",
    "# ############## Model saving and resoration ends! ################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here system will be trained with scibert\n",
    "\n",
    "# start_time = time.perf_counter()\n",
    "# data = pd.read_csv(\"EDLC_ner_dataset_V3.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# data.head(5)\n",
    "\n",
    "# class SentenceGetter(object):\n",
    "\n",
    "#     def __init__(self, data):\n",
    "#         self.n_sent = 1\n",
    "#         self.data = data\n",
    "#         self.empty = False\n",
    "#        # agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "#        #                                                    s[\"POS\"].values.tolist(),\n",
    "#         #                                                   s[\"Tag\"].values.tolist())]\n",
    "#         agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "#                                                            s[\"Tag\"].values.tolist())]\n",
    "#         self.grouped = self.data.groupby(\"SentenceID\").apply(agg_func)\n",
    "#         self.sentences = [s for s in self.grouped]\n",
    "\n",
    "#     def get_next(self):\n",
    "#         try:\n",
    "#             s = self.grouped[\"{}\".format(self.n_sent)]\n",
    "#             self.n_sent += 1\n",
    "#             return s\n",
    "#         except:\n",
    "#             return None\n",
    "\n",
    "# getter = SentenceGetter(data)\n",
    "# sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
    "# sentences[1]\n",
    "\n",
    "# labels = [[s[1] for s in sentence] for sentence in getter.sentences]\n",
    "# print(labels[1])\n",
    "\n",
    "# tag_values = list(set(data[\"Tag\"].values))\n",
    "# tag_values.append(\"PAD\")\n",
    "# tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "\n",
    "# #Preprocess the sentence nd labels. prepare to use with pytorch and bert\n",
    "\n",
    "# #batch size = bs\n",
    "# # sentence length fixed to 50 i.e. 50 tokens as of EDA it has been observed that in the dataset max length it 55 and most of them\n",
    "# # are of O tags\n",
    "# # but bert supports up to 512 tokens \n",
    "# #As the current machine config AMD Ryzen 7 4800H and NVidia Geforce GTX1650 (4GB), It gives error for larger batch size \n",
    "# #Hence batch size is limited to 16 for both our DNN model and for BERT pretrained model\n",
    "\n",
    "\n",
    "# MAX_LEN = 50\n",
    "# bs = 4\n",
    "\n",
    "# #GPU CPU setup for the training device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# ##uncomment this line to use with GPU, default is set for CPU\n",
    "# # torch.cuda.get_device_name(0)\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('scibert_scivocab_uncased', do_lower_case=False) \n",
    "\n",
    "# #uncased better for for ner using SciBERT as of their published paper\n",
    "\n",
    "# def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "#     tokenized_sentence = []\n",
    "#     labels = []\n",
    "\n",
    "#     for word, label in zip(sentence, text_labels):\n",
    "\n",
    "#         # Tokenize the word and count # of subwords the word is broken into\n",
    "#         tokenized_word = tokenizer.tokenize(word)\n",
    "#         n_subwords = len(tokenized_word)\n",
    "\n",
    "#         # Add the tokenized word to the final tokenized word list\n",
    "#         tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "#         # Add the same label to the new list of labels `n_subwords` times\n",
    "#         labels.extend([label] * n_subwords)\n",
    "\n",
    "#     return tokenized_sentence, labels\n",
    "\n",
    "# tokenized_texts_and_labels = [\n",
    "#     tokenize_and_preserve_labels(sent, labs)\n",
    "#     for sent, labs in zip(sentences, labels)\n",
    "# ]\n",
    "\n",
    "# tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "# labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "\n",
    "# #cut and pad to the desied length 50 for the smaller length sentences\n",
    "# input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "#                           maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "#                           truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "# tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "#                      maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "#                      dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "# #attenation mask to ignore PAD token\n",
    "# attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "\n",
    "# #10per train and validATE\n",
    "# tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
    "#                                                             random_state=2018, test_size=0.1)\n",
    "# tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "#                                              random_state=2018, test_size=0.1)\n",
    "\n",
    "# # convert to torch tenors\n",
    "# tr_inputs = torch.tensor(tr_inputs)\n",
    "# val_inputs = torch.tensor(val_inputs)\n",
    "# tr_tags = torch.tensor(tr_tags)\n",
    "# val_tags = torch.tensor(val_tags)\n",
    "# tr_masks = torch.tensor(tr_masks)\n",
    "# val_masks = torch.tensor(val_masks)\n",
    "\n",
    "\n",
    "# #training time shuffling of the data and testing time we pass them sequentially\n",
    "# train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "# valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "# valid_sampler = SequentialSampler(valid_data)\n",
    "# valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n",
    "\n",
    "# #Using pretrained BERT model (12-layer, 768-hidden, 12-heads, 110M parameters)\n",
    "# # Source: https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/\n",
    "# # https://github.com/google-research/bert\n",
    "\n",
    "# model = BertForTokenClassification.from_pretrained(\n",
    "#     \"scibert_scivocab_uncased\",\n",
    "#     num_labels=len(tag2idx),\n",
    "#     output_attentions = False,\n",
    "#     output_hidden_states = False\n",
    "# )\n",
    "\n",
    "# ## Uncomment the following line use with GPU, comment to use with CPU\n",
    "# # model.cuda();\n",
    "\n",
    "# #Using optimizers: \"AdamW\"\n",
    "\n",
    "# FULL_FINETUNING = True\n",
    "# if FULL_FINETUNING:\n",
    "#     param_optimizer = list(model.named_parameters())\n",
    "#     no_decay = ['bias', 'gamma', 'beta']\n",
    "#     optimizer_grouped_parameters = [\n",
    "#         {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "#          'weight_decay_rate': 0.01},\n",
    "#         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "#          'weight_decay_rate': 0.0}\n",
    "#     ]\n",
    "# else:\n",
    "#     param_optimizer = list(model.classifier.named_parameters())\n",
    "#     optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "# optimizer = AdamW(\n",
    "#     optimizer_grouped_parameters,\n",
    "#     lr=3e-5,\n",
    "#     eps=1e-8\n",
    "# )\n",
    "\n",
    "# #schduler to reduce learning rate linearly throughout the epochs\n",
    "# # with gpu epochs is 30, in CPU it is 10 just to test the working condition and save time. For production use, epochs should be 30 or more\n",
    "# epochs = 10\n",
    "# max_grad_norm = 1.0\n",
    "\n",
    "# # Total number of training steps is number of batches * number of epochs.\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# # Create the learning rate scheduler.\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=total_steps\n",
    "# )\n",
    "\n",
    "# def flat_accuracy(preds, labels):\n",
    "#     pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "#     labels_flat = labels.flatten()\n",
    "#     return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# # Fitting the BERT model for NER task\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# ## Store the average loss after each epoch to plot them later\n",
    "# loss_values, validation_loss_values = [], []\n",
    "\n",
    "# for _ in trange(epochs, desc=\"Epoch\"):\n",
    "#     # ========================================\n",
    "#     #               Training\n",
    "#     # ========================================\n",
    "#     # Perform one full pass over the training set.\n",
    "\n",
    "#     # Put the model into training mode.\n",
    "#     model.train()\n",
    "#     # Reset the total loss for this epoch.\n",
    "#     total_loss = 0\n",
    "# #     total_loss = torch.tensor.numpy()\n",
    "\n",
    "#     # Training loop\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         # add batch to gpu\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "#         #following 3 lines are fix for windows setup computation | converting to 64bit int\n",
    "#         b_input_ids = torch.tensor(b_input_ids).to(torch.int64)\n",
    "#         b_input_mask = torch.tensor(b_input_mask).to(torch.int64)\n",
    "#         b_labels = torch.tensor(b_labels).to(torch.int64)\n",
    "#         # Always clear any previously calculated gradients before performing a backward pass.\n",
    "#         model.zero_grad()\n",
    "#         # forward pass\n",
    "#         # This will return the loss (rather than the model output)\n",
    "#         # because we have provided the `labels`.\n",
    "#         outputs = model(b_input_ids, token_type_ids=None,\n",
    "#                         attention_mask=b_input_mask, labels=b_labels)\n",
    "#         # get the loss\n",
    "#         loss = outputs[0]\n",
    "#         # Perform a backward pass to calculate the gradients.\n",
    "#         loss.mean().backward()\n",
    "#         # track train loss\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "#         # Clip the norm of the gradient\n",
    "#         # This is to help prevent the \"exploding gradients\" problem.\n",
    "#         torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "#         # update parameters\n",
    "#         optimizer.step()\n",
    "#         # Update the learning rate.\n",
    "#         scheduler.step()\n",
    "\n",
    "#     # Calculate the average loss over the training data.\n",
    "#     avg_train_loss = total_loss / len(train_dataloader)\n",
    "#     print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "#     # Store the loss value for plotting the learning curve.\n",
    "#     loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "#     # ========================================\n",
    "#     #               Validation\n",
    "#     # ========================================\n",
    "#     # After the completion of each training epoch, measure our performance on\n",
    "#     # our validation set.\n",
    "\n",
    "#     # Put the model into evaluation mode\n",
    "#     model.eval()\n",
    "#     # Reset the validation loss for this epoch.\n",
    "#     eval_loss, eval_accuracy = 0, 0\n",
    "#     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "#     predictions , true_labels = [], []\n",
    "#     for batch in valid_dataloader:\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "#         #following 3 lines are fix for windows setup computation | converting to 64bit int\n",
    "#         b_input_ids = torch.tensor(b_input_ids).to(torch.int64)\n",
    "#         b_input_mask = torch.tensor(b_input_mask).to(torch.int64)\n",
    "#         b_labels = torch.tensor(b_labels).to(torch.int64)\n",
    "#         # Telling the model not to compute or store gradients,\n",
    "#         # saving memory and speeding up validation\n",
    "#         with torch.no_grad():\n",
    "#             # Forward pass, calculate logit predictions.\n",
    "#             # This will return the logits rather than the loss because we have not provided labels.\n",
    "#             outputs = model(b_input_ids, token_type_ids=None,\n",
    "#                             attention_mask=b_input_mask,labels=b_labels)\n",
    "#         # Move logits and labels to CPU\n",
    "#         logits = outputs[1].detach().cpu().numpy()\n",
    "#         label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "#         # Calculate the accuracy for this batch of test sentences.\n",
    "#         eval_loss += outputs[0].mean().item()\n",
    "#         eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "#         predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "# #         predictions.extend([list(logits[0])])\n",
    "#         true_labels.extend(label_ids)\n",
    "\n",
    "#         nb_eval_examples += b_input_ids.size(0)\n",
    "#         nb_eval_steps += 1\n",
    "\n",
    "#     eval_loss = eval_loss / nb_eval_steps\n",
    "#     validation_loss_values.append(eval_loss)\n",
    "#     print(\"Validation loss: {}\".format(eval_loss))\n",
    "#     print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "#     pred_tags = [[tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "#                                  for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]]\n",
    "#     valid_tags = [[tag_values[l_i] for l in true_labels\n",
    "#                                   for l_i in l if tag_values[l_i] != \"PAD\"]]\n",
    "#     print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "#     print(classification_report(pred_tags,valid_tags))\n",
    "#     print(\"Finished epoch!\")\n",
    "\n",
    "    \n",
    "# ## Plotting disabled for this program, can be found in the ML models implementation    \n",
    "# # # Use plot styling from seaborn.\n",
    "# # sns.set(style='darkgrid')\n",
    "\n",
    "# # # Increase the plot size and font size.\n",
    "# # sns.set(font_scale=1.5)\n",
    "# # plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "\n",
    "# # # Plot the learning curve.\n",
    "# # plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
    "# # plt.plot(validation_loss_values, 'r-o', label=\"validation loss\")\n",
    "\n",
    "# # # Label the plot.\n",
    "# # plt.title(\"Train Test validation loss\")\n",
    "# # plt.xlabel(\"Epoch\")\n",
    "# # plt.ylabel(\"Loss\")\n",
    "# # plt.legend()\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "# print (time.perf_counter() - start_time, \"seconds taken to train the system in cpu mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # global_token_list_mat, global_label_list_mat,global_token_list_proc, global_label_list_proc,global_token_list_val, global_label_list_val, global_sent_list = [], [], [],[],[],[],[]\n",
    "# # global_token_list, global_label_list =[],[]\n",
    "# # global_batil_list = ['[SEP]','[CLS]','[UNK]','+','=','Of','In','At','-','Indeed','I','II','III','IV']\n",
    "# def predict_sentence(sent,file_name):\n",
    "# #     sent = re.sub(r\"\\.\",\"\",sent)\n",
    "#     sent = re.sub(r\"\\/\",\"\",sent)\n",
    "#     sent = re.sub(r\"Á\",\"ση\",sent)\n",
    "#     sent = re.sub(r\"•\",\"°\",sent)\n",
    "#     tokenized_sentence = tokenizer.encode(sent)\n",
    "#     ##Uncomment the following line to use with GPU\n",
    "# #     input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "#     ##Comment the following line to use with GPU\n",
    "#     input_ids = torch.tensor([tokenized_sentence])\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         output = model(input_ids)\n",
    "#     label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "\n",
    "\n",
    "#     # join bpe split tokens\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "#     new_tokens, new_labels = [], []\n",
    "#     for token, label_idx in zip(tokens, label_indices[0]):\n",
    "#         if token.startswith(\"##\"):\n",
    "#             new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "#         else:\n",
    "#             new_labels.append(tag_values[label_idx])\n",
    "#             new_tokens.append(token)\n",
    "\n",
    "# #     print(\"Listing number by spacy\\n\")\n",
    "# #     for tok in new_tokens:\n",
    "# #         toks = nlp(tok)\n",
    "# #         for token in toks:\n",
    "# #             if token.like_num:\n",
    "# #                 print (token.text)\n",
    "# #             else:\n",
    "# #                 for ent in toks.ents:\n",
    "# #                     print(ent.text,\" not a number but \", ent.label_)\n",
    "#     tok_index = len(new_tokens)\n",
    "#     label_index = len(new_labels)\n",
    "# #     print(new_tokens)\n",
    "# #     print('-----------')\n",
    "    \n",
    "#     prev_label_mat = ''\n",
    "#     prev_label_proc = ''\n",
    "#     prev_label_val = ''\n",
    "#     current_label = ''\n",
    "#     prev_toekn_mat = ''\n",
    "#     prev_toekn_proc = ''\n",
    "#     prev_toekn_val = ''\n",
    "#     current_token=''\n",
    "    \n",
    "#     for token, label in zip(new_tokens, new_labels):\n",
    "        \n",
    "        \n",
    "# #         for tok in new_tokens:\n",
    "# #         toks = nlp(token)\n",
    "# #         for token_spacy in toks:\n",
    "# #             if token_spacy.like_num:\n",
    "# #                 print (token_spacy.text)\n",
    "# #             else:\n",
    "# #                 for ent in toks.ents:\n",
    "# #                     print(ent.text,\" not a number but \", ent.label_)\n",
    "#         if label in ['B-mat','I-mat','B-proc','I-proc','B-val','I-val'] and token not in global_batil_list:\n",
    "#             current_label = label\n",
    "#             current_token= token\n",
    "#             global_sent_list.append(sent)\n",
    "            \n",
    "#             if prev_label_mat == '' and current_label =='B-mat':\n",
    "#                 prev_label_mat = current_label\n",
    "#                 prev_token_mat = current_token\n",
    "# #                 print(prev_token_mat)\n",
    "#             if prev_label_mat != '' and current_label =='I-mat':\n",
    "#                 prev_label_mat = prev_label_mat + \" \" + current_label\n",
    "#                 prev_token_mat = prev_token_mat + \" \" + current_token\n",
    "# #                 print(prev_token_mat)\n",
    "#             if prev_label_mat != '' and current_label =='B-mat':\n",
    "#                 global_label_list_mat.append(prev_label_mat)\n",
    "#                 global_token_list_mat.append(prev_token_mat)\n",
    "#                 global_label_list.append(prev_label_mat)\n",
    "#                 global_token_list.append(prev_token_mat)\n",
    "# #                 print(prev_token_mat)\n",
    "#                 prev_label_mat=current_label\n",
    "#                 prev_token_mat = current_token\n",
    "                \n",
    "#             if prev_label_proc == '' and current_label =='B-proc':\n",
    "#                 prev_label_proc = current_label\n",
    "#                 prev_token_proc = current_token\n",
    "# #                 print(prev_token_proc)\n",
    "#             if prev_label_proc != '' and current_label =='I-proc':\n",
    "#                 prev_label_proc = prev_label_proc + \" \" + current_label\n",
    "#                 prev_token_proc = prev_token_proc + \" \" + current_token\n",
    "# #                 print(prev_token_proc)\n",
    "#             if prev_label_proc != '' and current_label =='B-proc':\n",
    "#                 global_label_list_proc.append(prev_label_proc)\n",
    "#                 global_token_list_proc.append(prev_token_proc)\n",
    "#                 global_label_list.append(prev_label_proc)\n",
    "#                 global_token_list.append(prev_token_proc)\n",
    "# #                 print(prev_token_proc)\n",
    "#                 prev_label_proc=current_label\n",
    "#                 prev_token_proc = current_token\n",
    "                \n",
    "#             if prev_label_val == '' and current_label =='B-val':\n",
    "#                 prev_label_val = current_label\n",
    "#                 prev_token_val = current_token\n",
    "# #                 print(prev_token_val)\n",
    "#             if prev_label_val != '' and current_label =='I-val':\n",
    "#                 prev_label_val = prev_label_val + \" \" + current_label\n",
    "#                 prev_token_val = prev_token_val + \" \" + current_token\n",
    "# #                 print(prev_token_val)\n",
    "#             if prev_label_val != '' and current_label =='B-val':\n",
    "#                 global_label_list_val.append(prev_label_val)\n",
    "#                 global_token_list_val.append(prev_token_val)\n",
    "#                 global_label_list.append(prev_label_val)\n",
    "#                 global_token_list.append(prev_token_val)\n",
    "# #                 print(prev_token_val)\n",
    "#                 prev_label_val=current_label\n",
    "#                 prev_token_val = current_token\n",
    "        \n",
    "# #         if re.match(r'\\d',token) or re.match(r'\\d+\\.*\\d*',token):\n",
    "#         if re.match(r'\\d+\\.*\\d*',token):\n",
    "#             sent_analyser.write(file_name+\"\\t\"+token+\"\\t\"+label+\"\\t\"+sent+\"\\n\")\n",
    "\n",
    "# #             print(\"{:20}\\t{}\".format(current_token, current_label))\n",
    "# #             print(\"\\n---------------------------------\")\n",
    "\n",
    "        \n",
    "# # sent1= \"Hierarchical Porous Nitrogen-Doped Carbon Nanosheets Derived from Silk for Ultrahigh-Capacity Battery Anodes and Supercapacitors\"\n",
    "# # sent2 = \"Li metal foil as the counter electrode and 1 M LiPF6 solution with ethylene carbonate-dimethyl carbonate as the electrolyte\" \n",
    "\n",
    "# # predict_sentence(sent1)\n",
    "# # print(\"----------------------------------------\")\n",
    "# # predict_sentence(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path varibale defines from where the text files will be loaded\n",
    "file_path = \"test-papers-text\"\n",
    "def get_filenames(file_extension):\n",
    "    files = []\n",
    "    for r,d, f in os.walk(file_path+\"/\"):\n",
    "        for file in f:\n",
    "            if \".\"+file_extension in file:\n",
    "                files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"(\\[[0-9].?\\])|(\\[[0-9].?.\\])|(\\[[0-9][0-9],[0-9][0-9]\\])\",\"\",text)\n",
    "    text = re.sub(r\"\\s{2,}\",\" \",text)\n",
    "    text = re.sub(r\"[A-Z]\\s[A-Z]\",\"\",text)\n",
    "    text = re.sub(r\"(?<=[A-Z].)\\s(?=[0-9])|(?<=[0-9])\\s(?=[A-Z])|(?<=[A-Z])\\s(?=[0-9])|(?<=\\d)\\s(?=\\d)\",\"\",text)\n",
    "    text = re.sub(r\"\\(\",\"\",text)\n",
    "    text = re.sub(r\"\\)\",\"\",text)\n",
    "    text = re.sub(r\"\\[\",\"\",text)\n",
    "    text = re.sub(r\"\\]\",\"\",text)\n",
    "    text = re.sub(r\",\",\"\",text)\n",
    "    text = re.sub(r\";\",\"\",text)\n",
    "    text = re.sub(r\"~\",\"\",text)\n",
    "    text = re.sub(r\"'\",\"\",text)\n",
    "    text = re.sub(r\"\\sÀ\",\"-\",text)\n",
    "    text = re.sub(r\"À\",\"-\",text)\n",
    "    text = re.sub(r\"Á\",\"ση\",text)\n",
    "    text = re.sub(r\"•\",\"°\",text)\n",
    "    text = re.sub(r'[‘’“”…]', '', text)\n",
    "    text = re.sub(r\"Figure\", '', text)\n",
    "#     text = re.sub('\\[.*?\\]', '', text)\n",
    "#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "#     text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here the appropriate paper checking functions will be placed\n",
    "def get_cosine_sim(doc1,doc2):\n",
    "    X_list = word_tokenize(doc1)  \n",
    "    Y_list = word_tokenize(doc2) \n",
    "    sw = stopwords.words('english')  \n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # remove stop words from the string \n",
    "    X_set = {w for w in X_list if not w in sw}  \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "#     print(\"similarity: \", cosine)\n",
    "    return cosine\n",
    "\n",
    "def get_jaccard_sim(doc1, doc2): \n",
    "    a = set(doc1.split()) \n",
    "    b = set(doc2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def get_wv_sim(doc1,doc2):\n",
    "    doc1 = nlp(doc1)\n",
    "    doc2 = nlp(doc2)\n",
    "    # Similarity of two documents\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "\n",
    "def get_primary_kw_sim(text):\n",
    "    doc2_f = open(\"primary-kw.txt\",\"r\",encoding = \"utf-8\")\n",
    "    doc1 = text\n",
    "    doc2 = doc2_f.read()\n",
    "#     doc1 = clean_text(doc1)\n",
    "    doc2 = clean_text(doc2)\n",
    "    jaccard_sim = get_jaccard_sim(doc1,doc2)\n",
    "    cosine_sim = get_cosine_sim(doc1,doc2)\n",
    "    wv_sim = get_wv_sim(doc1,doc2)\n",
    "    doc2_f.close()\n",
    "    return jaccard_sim, cosine_sim, wv_sim\n",
    "\n",
    "\n",
    "def get_secondary_kw_sim(text):\n",
    "    doc2_f = open(\"secondary-kw.txt\",\"r\",encoding = \"utf-8\")\n",
    "    doc1 = text\n",
    "    doc2 = doc2_f.read()\n",
    "#     doc1 = clean_text(doc1)\n",
    "    doc2 = clean_text(doc2)\n",
    "    jaccard_sim = get_jaccard_sim(doc1,doc2)\n",
    "    cosine_sim = get_cosine_sim(doc1,doc2)\n",
    "    wv_sim = get_wv_sim(doc1,doc2)\n",
    "    doc2_f.close()\n",
    "    return jaccard_sim, cosine_sim, wv_sim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## for current get file method, need to put only extension of file\n",
    "## then get full filename using\n",
    "## filename = path+file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "files_list = get_filenames('txt')\n",
    "\n",
    "# for files in files_list:\n",
    "#     print (files)\n",
    "print(len(files_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from nltk.tokenize import sent_tokenize \n",
    "# ## Open 🔓 global knowledge_graph_nt_writer\n",
    "# # kg_nt_writer = open(\"Material_kG.nt\",\"a+\",encoding=\"utf-8\")\n",
    "# sent_analyser = open(\"sentence_analysis_TSV_spacy.tsv\",\"a+\",encoding=\"utf-8\")\n",
    "# for file in files_list:\n",
    "#     global_token_list_mat, global_label_list_mat,global_token_list_proc, global_label_list_proc,global_token_list_val, global_label_list_val, global_sent_list = [], [], [],[],[],[],[]\n",
    "#     global_token_list, global_label_list =[],[]\n",
    "#     global_batil_list = ['[SEP]','[CLS]','[UNK]','+','=','Of','In','At','-','Indeed','I','II','III','IV']\n",
    "\n",
    "#     file_handler = open(file_path+\"/\"+file, \"r\", encoding=\"utf-8\")\n",
    "#     text_content = file_handler.read()\n",
    "#     paper_doi = text_content.split(\"\\n\")[1]\n",
    "#     text_content = clean_text(text_content)\n",
    "    \n",
    "#     ## Check appropriate paper for primary and secondary keyword\n",
    "# #     primary_similarity = get_primary_kw_sim(text_content)\n",
    "# #     secondary_similarity = get_secondary_kw_sim(text_content)\n",
    "#         ## Need to discuss about the values of PS and SS (PS->PrimarySimilarity | SS->Secondary Similarity)\n",
    "# #     if primary_similarity >= and secondary_similarity >= :\n",
    "# #     paper_doi = text_content.split(\"\\n\")[1]\n",
    "#     print(\"##############################################\")\n",
    "#     print(f\"Current file: {file} and Doi: {paper_doi}\")\n",
    "#     print(\"##############################################\")\n",
    "    \n",
    "# #     print(text_content.split(\"\\n\")[1])\n",
    "# #     print(type(text_content))\n",
    "\n",
    "    \n",
    "# #     print(type(text_content))\n",
    "\n",
    "#     sent_list = sent_tokenize(text_content)\n",
    "# #     sents_spacy = nlp(text_content)\n",
    "# #     sent_list_spacy = [sent.text for sent in sents_spacy.sents]\n",
    "#     # print(sentence_list[0])\n",
    "#     # print(\"--------------\")\n",
    "#     # print(sent_list_spacy[0])\n",
    "#     # print(len(word_tokenize(sentence_list[0])))\n",
    "#     # print(len(nlp.tokenizer(sent_list_spacy[0])))\n",
    "#     # print(word_tokenize(sentence_list[0]))\n",
    "    \n",
    "#     sw_spacy = nlp.Defaults.stop_words\n",
    "\n",
    "#     # words_temp = [word for word in word_tokenize(sent_list_spacy[0]) if word not in sw_spacy]\n",
    "#     # words_temp = [word for word in word_tokenize(sentence_list) if word not in sw_spacy]\n",
    "#     # new_text = \" \".join(words_temp)\n",
    "#     # new_text = re.sub(r\"\\.\",\"\",new_text)\n",
    "#     # print(new_text)\n",
    "#     # # This function is the prediction function for lstm method\n",
    "#     # create_test_input_from_text(new_text)\n",
    "#     # prediction function for sciBERT Transformer model\n",
    "#     # predict_sentence(new_text)\n",
    "\n",
    "\n",
    "#     count = 1\n",
    "#     for sentence in sent_list:\n",
    "#         print(\"sentence: \",count)\n",
    "#         count+=1\n",
    "#         words_temp = [word for word in word_tokenize(sentence) if word not in sw_spacy]\n",
    "#         new_text = \" \".join(words_temp)\n",
    "#     #     print(new_text,\"\\n-------------\\n\")\n",
    "#         predict_sentence(new_text,file)\n",
    "\n",
    "\n",
    "#     # print(global_sent_list)    \n",
    "\n",
    "# #     for t,l in zip(global_token_list, global_label_list):\n",
    "# #         print(\"{:20}\\t{}\".format(t, l))\n",
    "# #         print(\"\\n---------------------------------\\n\")\n",
    "\n",
    "# #     print (global_sent_list)\n",
    "#     print (\"\\n\\n ##### Processed file: \",file,\"DOI: \",paper_doi, \"\\n\\n\")\n",
    "    \n",
    "# #     else:\n",
    "# #         Print(\"NOTICE!:: Doc similarity is not enough to process this text file\", file)\n",
    "\n",
    "# sent_analyser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processed:  ['Electroless', 'decoration', 'of', 'cellulose'].tei.xml.txt\n",
      "Processed:  ['Experimental', 'investigation', 'into', 'the'].tei.xml.txt\n",
      "Processed:  ['Derivation', 'of', 'both', 'EDLC'].tei.xml.txt\n",
      "Processed:  ['Tailoring', 'morphology', 'to', 'control'].tei.xml.txt\n",
      "ipykernel_launcher:38: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "Processed:  ['zhan2020'].tei.xml.txt\n",
      "Processed:  ['Influence', 'of', '$$', 'hbox'].tei.xml.txt\n",
      "Processed:  ['High-performance', 'hybrid', 'supercapacitor', 'of'].tei.xml.txt\n",
      "Processed:  ['Protonic', 'EDLC', 'cell', 'based'].tei.xml.txt\n",
      "Processed:  ['Chemical', 'and', 'structural', 'optimization'].tei.xml.txt\n",
      "Processed:  ['Novel', 'chemical', 'route', 'for'].tei.xml.txt\n",
      "Processed:  ['High', 'Proton', 'Conducting', 'Polymer'].tei.xml.txt\n",
      "Processed:  ['Biomass-Based', 'N,', 'P,', 'and'].tei.xml.txt\n",
      "Processed:  ['Fabrication', 'of', 'high', 'performance'].tei.xml.txt\n",
      "Processed:  ['noh2019'].tei.xml.txt\n",
      "Processed:  ['Hierarchically', 'porous', 'carbon', 'derived'].tei.xml.txt\n",
      "Processed:  ['Enhanced', 'electrochemical', 'studies', 'of'].tei.xml.txt\n",
      "Processed:  ['913391ea5cdb0aa0f942fe964b173428'].tei.xml.txt\n",
      "Processed:  ['Electrochemical', 'Double-Layer', 'Capacitor', 'Energized'].tei.xml.txt\n",
      "Processed:  ['muchakayala2017'].tei.xml.txt\n",
      "Processed:  ['Role', 'of', 'nano-capacitor', 'on'].tei.xml.txt\n",
      "Processed:  ['Template-induced', 'self-activation', 'route', 'for'].tei.xml.txt\n",
      "Processed:  ['Nontraditional,', 'Safe,', 'High', 'Voltage'].tei.xml.txt\n",
      "Processed:  ['Pinecone-derived', 'porous', 'activated', 'carbon'].tei.xml.txt\n",
      "Processed:  ['Facile', 'Synthesis', 'of', 'Nitrogen-Doped'].tei.xml.txt\n",
      "Processed:  ['Frame-filling', 'C', 'C', 'composite'].tei.xml.txt\n",
      "Processed:  ['A', 'review', 'on', 'the'].tei.xml.txt\n",
      "Processed:  ['Non-aqueous', 'electrolytes', 'for', 'electrochemical'].tei.xml.txt\n",
      "Processed:  ['Lithium', 'Ion', 'Capacitors', 'in'].tei.xml.txt\n",
      "Processed:  ['db21a70303d1e33f79fd2aca9be8a457'].tei.xml.txt\n",
      "Processed:  ['Effect', 'of', 'glycerol', 'on'].tei.xml.txt\n",
      "Processed:  ['Influence', 'of', 'porosity', 'parameters'].tei.xml.txt\n",
      "Processed:  ['Graphene', 'oxide', '–', 'Based'].tei.xml.txt\n",
      "Processed:  ['arnaiz2019'].tei.xml.txt\n",
      "Processed:  ['Study', 'of', 'the', 'electrical'].tei.xml.txt\n",
      "Processed:  ['Hierarchical', 'nanocarbon-MnO', '2', 'electrodes'].tei.xml.txt\n",
      "Processed:  ['High', 'temperature', 'solid-state', 'supercapacitor'].tei.xml.txt\n",
      "Processed:  ['PVDF-HFP', 'and', '1-ethyl-3-methylimidazolium', 'thiocyanate–doped'].tei.xml.txt\n",
      "Processed:  ['Controlling', 'electric', 'double-layer', 'capacitance'].tei.xml.txt\n",
      "Processed:  ['Comparison', 'of', 'ionic', 'liquid'].tei.xml.txt\n",
      "Processed:  ['Fabrication', 'of', 'energy', 'storage'].tei.xml.txt\n",
      "Processed:  ['Perspective', 'on', 'electrochemical', 'capacitor'].tei.xml.txt\n",
      "Processed:  ['Nickel-cobalt', 'phosphate', 'graphene', 'foam'].tei.xml.txt\n",
      "Processed:  ['Helically', 'coiled', 'carbon', 'nanotubes'].tei.xml.txt\n",
      "Processed:  ['Waste', 'plastic', 'derived', 'carbon'].tei.xml.txt\n",
      "Processed:  ['A', 'Promising', 'Polymer', 'Blend'].tei.xml.txt\n",
      "Processed:  ['High', 'Performance', 'Poly(vinyl', 'alcohol)-Based'].tei.xml.txt\n",
      "Processed:  ['Biopolymeric', 'electrolyte', 'based', 'on'].tei.xml.txt\n",
      "Processed:  ['Influence', 'of', 'graphitic', 'amorphous'].tei.xml.txt\n",
      "Processed:  ['Metal', 'Framework', 'as', 'a'].tei.xml.txt\n",
      "Processed:  ['Electrochemical', 'Characteristics', 'of', 'Glycerolized'].tei.xml.txt\n",
      "Processed:  ['Effect', 'of', 'ohmic-drop', 'on'].tei.xml.txt\n",
      "Processed:  ['Laser-Induced', 'Reduction', 'of', 'Graphene'].tei.xml.txt\n",
      "Processed:  ['Battery-like', 'Supercapacitors', 'from', 'Vertically'].tei.xml.txt\n",
      "Processed:  ['Plasticized', 'solid', 'polymer', 'electrolyte'].tei.xml.txt\n",
      "Processed:  ['A', 'critical', 'review', 'on'].tei.xml.txt\n",
      "Processed:  ['Electrochemical', 'analysis', 'of', 'nanoporous'].tei.xml.txt\n",
      "Processed:  ['Glycerolized', 'Li+', 'Ion', 'Conducting'].tei.xml.txt\n",
      "Processed:  ['Boost-up', 'Electrochemical', 'Performance', 'of'].tei.xml.txt\n",
      "Processed:  ['Waste', 'driven', 'Bio-Carbon', 'Electrode'].tei.xml.txt\n",
      "Processed:  ['Synthesis', 'of', 'Reduced', 'Graphene'].tei.xml.txt\n",
      "Processed:  ['Facile', 'sono-chemical', 'synthesis', 'of'].tei.xml.txt\n",
      "Processed:  ['Current', 'progress', 'achieved', 'in'].tei.xml.txt\n",
      "Processed:  ['Core–shell', 'structural', 'PANI-derived', 'carbon@Co–Ni'].tei.xml.txt\n",
      "Processed:  ['Enhancing', 'the', 'performance', 'of'].tei.xml.txt\n",
      "Processed:  ['A', 'comparative', 'study', 'of'].tei.xml.txt\n",
      "Processed:  ['Flexible', 'graphene‐carbon', 'nanotube', 'electrochemical'].tei.xml.txt\n",
      "Processed:  ['A', 'review', 'on', 'recent'].tei.xml.txt\n",
      "Processed:  ['Electrodes', 'and', 'hydrogel', 'electrolytes'].tei.xml.txt\n",
      "Processed:  ['Review', 'of', 'carbon-based', 'electrode'].tei.xml.txt\n",
      "Processed:  ['Solvent-free', 'synthesis', 'and', 'KOH'].tei.xml.txt\n",
      "Processed:  ['Investigation', 'of', 'plasticized', 'ionic'].tei.xml.txt\n",
      "Processed:  ['The', 'properties', 'and', 'performance'].tei.xml.txt\n",
      "Processed:  ['Designing', 'a', 'Novel', 'Polymer'].tei.xml.txt\n",
      "Processed:  ['e291c9bb502860360f26ee47aae0f27a'].tei.xml.txt\n",
      "Processed:  ['High', 'performance', 'quasi-solid-state', 'supercapacitors'].tei.xml.txt\n",
      "Processed:  ['Mesoporous', 'tubular', 'graphene', 'electrode'].tei.xml.txt\n",
      "Processed:  ['Development', 'of', 'Polymer', 'Blend'].tei.xml.txt\n",
      "Processed:  ['Construction', 'of', 'NiCo', '2'].tei.xml.txt\n",
      "Processed:  ['Electrochemical', 'characteristics', 'of', 'solid'].tei.xml.txt\n",
      "Processed:  ['Supercapacitors', 'Properties', 'and', 'applications'].tei.xml.txt\n",
      "Processed:  ['Porous', 'carbon', 'nanospheres', 'with'].tei.xml.txt\n",
      "Processed:  ['Enhanced', 'electrochemical', 'performance', 'for'].tei.xml.txt\n",
      "Processed:  ['Materials', 'mutualism', 'through', 'EDLC'].tei.xml.txt\n",
      "Processed:  ['Structural,', 'Impedance,', 'and', 'EDLC'].tei.xml.txt\n",
      "Processed:  ['Few', 'layer', 'graphene', 'wrapped'].tei.xml.txt\n",
      "Processed:  ['Ionic', 'Liquid', 'Modified', 'Poly(vinyl'].tei.xml.txt\n",
      "Processed:  ['Investigation', 'on', 'favourable', 'ionic'].tei.xml.txt\n",
      "Processed:  ['scalia2019'].tei.xml.txt\n",
      "Processed:  ['High-voltage', 'supercapacitors', 'based', 'on'].tei.xml.txt\n",
      "Processed:  ['Mesoporous', 'Carbons', 'Templated', 'by'].tei.xml.txt\n",
      "Processed:  ['Antimonene', 'A', 'Novel', '2D'].tei.xml.txt\n",
      "Processed:  ['Scalable', 'green', 'synthesis', 'of'].tei.xml.txt\n",
      "Processed:  ['How', 'and', 'where', 'to'].tei.xml.txt\n",
      "Processed:  ['antil2019'].tei.xml.txt\n",
      "Processed:  ['Metal', 'Complex', 'as', 'a'].tei.xml.txt\n",
      "Processed:  ['Solid', 'polymer', 'electrolytes', 'based'].tei.xml.txt\n",
      "Processed:  ['Cellulose', 'binders', 'for', 'electric'].tei.xml.txt\n",
      "Processed:  ['A', 'systematically', 'comparative', 'study'].tei.xml.txt\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pkw_list_fh = open(\"primary-kw.txt\",\"r\")\n",
    "pkw_list = pkw_list_fh.read()\n",
    "pkw_list = clean_text(pkw_list)\n",
    "\n",
    "sent_analyser = open(\"sentence_analysis_report_test_papers_positive_sent.tsv\",\"a+\",encoding=\"utf-8\")\n",
    "sent_analyser.write(\"file name\"+\"\\t\"+\"ps_count\"+\"\\t\"+\"eis_count\"+\"\\t\"+\"sc_count\"+\"\\t\"+\"t_score\"+\"\\t\"+\"wv_sim_score_all_sent\"+\"\\t\"+\"wv_sim_score_positive_sent\"+\"\\n\")\n",
    "for file in files_list:\n",
    "    t_score = 0\n",
    "    ps_count = 0\n",
    "    eis_count = 0\n",
    "    sc_count = 0\n",
    "    wv_sim_score = 0\n",
    "    # key_words = [\"pore size\",\"electrolyte ion size\", \"specific capacitance\"]\n",
    "    file_handler = open(file_path+\"/\"+file, \"r\", encoding=\"utf-8\")\n",
    "    text_content = file_handler.read()\n",
    "    text_content = clean_text(text_content)\n",
    "    \n",
    "    if text_content.count(\"pore size\") >= 1:\n",
    "        ps_count = text_content.count(\"pore size\")\n",
    "        t_score += 0.33\n",
    "    if text_content.count(\"electrolyte ion size\") >= 1:\n",
    "        eis_count = text_content.count(\"electrolyte ion size\")\n",
    "        t_score += 0.33\n",
    "    if text_content.count(\"specific capacitance\") >= 1:\n",
    "        sc_count = text_content.count(\"specific capacitance\")\n",
    "        t_score += 0.33\n",
    "\n",
    "    # if t_score < 0.99:\n",
    "    wv_sim_score_all = get_wv_sim(text_content,pkw_list)\n",
    "\n",
    "    neg_word_list = [\"never\",\"no\",\"hardly\",\"nobody\",\"none\",\"scarcely\",\"barely\",\"not\",\"no one\",\"nowhere\",\"rarely\"]\n",
    "    text_content_string = \"\"\n",
    "    text_content_string_neg = \"\"\n",
    "    sent_list = sent_tokenize(text_content)\n",
    "    for sent in sent_list:\n",
    "        if sent.count(neg_word_list[0])>=1 or  sent.count(neg_word_list[1])>=1 or sent.count(neg_word_list[2])>=1 or sent.count(neg_word_list[3])>=1 or sent.count(neg_word_list[4])>=1 or sent.count(neg_word_list[5])>=1 or sent.count(neg_word_list[6])>=1 or sent.count(neg_word_list[7])>=1 or sent.count(neg_word_list[8])>=1 or sent.count(neg_word_list[9])>=1 or sent.count(neg_word_list[10])>=1:\n",
    "            text_content_string_neg += sent\n",
    "        else:\n",
    "            text_content_string += sent\n",
    "    wv_sim_score_pos = get_wv_sim(text_content_string,pkw_list)\n",
    "    \n",
    "    sent_analyser.write(file+\"\\t\"+str(ps_count)+\"\\t\"+str(eis_count)+\"\\t\"+str(sc_count)+\"\\t\"+str(t_score)+\"\\t\"+str(wv_sim_score_all)+\"\\t\"+str(wv_sim_score_pos)+\"\\n\")\n",
    "    print(\"Processed: \",file)\n",
    "\n",
    "\n",
    "\n",
    "        ##Need to analyze each sentence for negative words\n",
    "        ## Then remove those sentence and concatenate rest sentence into a string and run\n",
    "        ## word vector similarity and save the result side by side for comparison\n",
    "\n",
    "'''\n",
    "Negative word reference:\n",
    "https://saylordotorg.github.io/text_business-english-for-success/s08-02-negative-statements.html\n",
    "'''\n",
    "sent_analyser.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "file_path = \"primary-papers\"\n",
    "files_list_p = get_filenames('txt')\n",
    "print(len(files_list_p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processed:  edlc73.tei.xml.txt\n",
      "Processed:  edlc31.tei.xml.txt\n",
      "Processed:  edlc41.tei.xml.txt\n",
      "Processed:  edlc101.tei.xml.txt\n",
      "Processed:  edlc24.tei.xml.txt\n",
      "Processed:  edlc54.tei.xml.txt\n",
      "Processed:  edlc66.tei.xml.txt\n",
      "Processed:  edlc16.tei.xml.txt\n",
      "Processed:  edlc108.tei.xml.txt\n",
      "Processed:  9.tei.xml.txt\n",
      "Processed:  edlc82.tei.xml.txt\n",
      "Processed:  edlc111.tei.xml.txt\n",
      "Processed:  edlc51.tei.xml.txt\n",
      "Processed:  edlc21.tei.xml.txt\n",
      "Processed:  edlc13.tei.xml.txt\n",
      "Processed:  edlc63.tei.xml.txt\n",
      "Processed:  edlc76.tei.xml.txt\n",
      "Processed:  edlc28.tei.xml.txt\n",
      "Processed:  edlc58.tei.xml.txt\n",
      "Processed:  5.tei.xml.txt\n",
      "Processed:  edlc97.tei.xml.txt\n",
      "Processed:  edlc94.tei.xml.txt\n",
      "Processed:  edlc47.tei.xml.txt\n",
      "Processed:  6.tei.xml.txt\n",
      "Processed:  edlc75.tei.xml.txt\n",
      "Processed:  edlc52.tei.xml.txt\n",
      "Processed:  edlc112.tei.xml.txt\n",
      "Processed:  edlc81.tei.xml.txt\n",
      "Processed:  edlc65.tei.xml.txt\n",
      "Processed:  edlc15.tei.xml.txt\n",
      "Processed:  edlc79.tei.xml.txt\n",
      "Processed:  edlc84.tei.xml.txt\n",
      "Processed:  edlc27.tei.xml.txt\n",
      "Processed:  edlc57.tei.xml.txt\n",
      "Processed:  edlc32.tei.xml.txt\n",
      "Processed:  edlc42.tei.xml.txt\n",
      "Processed:  3.tei.xml.txt\n",
      "Processed:  edlc91.tei.xml.txt\n",
      "Processed:  edlc70.tei.xml.txt\n",
      "Processed:  edlc35.tei.xml.txt\n",
      "Processed:  4.tei.xml.txt\n",
      "Processed:  edlc45.tei.xml.txt\n",
      "Processed:  edlc105.tei.xml.txt\n",
      "Processed:  10.tei.xml.txt\n",
      "Processed:  edlc59.tei.xml.txt\n",
      "Processed:  edlc29.tei.xml.txt\n",
      "Processed:  edlc62.tei.xml.txt\n",
      "Processed:  edlc12.tei.xml.txt\n",
      "Processed:  edlc83.tei.xml.txt\n",
      "Processed:  edlc20.tei.xml.txt\n",
      "Processed:  edlc50.tei.xml.txt\n",
      "Processed:  edlc110.tei.xml.txt\n",
      "Processed:  edlc17.tei.xml.txt\n",
      "Processed:  edlc39.tei.xml.txt\n",
      "Processed:  edlc49.tei.xml.txt\n",
      "Processed:  8.tei.xml.txt\n",
      "Processed:  edlc109.tei.xml.txt\n",
      "Processed:  edlc55.tei.xml.txt\n",
      "Processed:  edlc86.tei.xml.txt\n",
      "Processed:  1.tei.xml.txt\n",
      "Processed:  edlc40.tei.xml.txt\n",
      "Processed:  edlc30.tei.xml.txt\n",
      "Processed:  edlc72.tei.xml.txt\n",
      "Processed:  edlc71.tei.xml.txt\n",
      "Processed:  edlc43.tei.xml.txt\n",
      "Processed:  edlc33.tei.xml.txt\n",
      "Processed:  edlc90.tei.xml.txt\n",
      "Processed:  edlc85.tei.xml.txt\n",
      "Processed:  edlc78.tei.xml.txt\n",
      "Processed:  edlc56.tei.xml.txt\n",
      "Processed:  edlc26.tei.xml.txt\n",
      "Processed:  edlc64.tei.xml.txt\n",
      "Processed:  edlc23.tei.xml.txt\n",
      "Processed:  edlc53.tei.xml.txt\n",
      "Processed:  edlc80.tei.xml.txt\n",
      "Processed:  edlc61.tei.xml.txt\n",
      "Processed:  edlc89.tei.xml.txt\n",
      "Processed:  edlc74.tei.xml.txt\n",
      "Processed:  edlc68.tei.xml.txt\n",
      "Processed:  edlc95.tei.xml.txt\n",
      "Processed:  edlc106.tei.xml.txt\n",
      "Processed:  7.tei.xml.txt\n",
      "Processed:  edlc46.tei.xml.txt\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "sent_analyser_p = open(\"sentence_analysis_report_primary_papers_positive_sent.tsv\",\"a+\",encoding=\"utf-8\")\n",
    "sent_analyser_p.write(\"file name\"+\"\\t\"+\"ps_count\"+\"\\t\"+\"eis_count\"+\"\\t\"+\"sc_count\"+\"\\t\"+\"t_score\"+\"\\t\"+\"wv_sim_score_all_sent\"+\"\\t\"+\"wv_sim_score_positive_sent\"+\"\\n\")\n",
    "# sent_analyser_p.write(\"file name\"+\"\\t\"+\"ps_count\"+\"\\t\"+\"eis_count\"+\"\\t\"+\"sc_count\"+\"\\t\"+\"t_score\"+\"\\t\"+\"wv_sim_score\"+\"\\n\")\n",
    "for file in files_list_p:\n",
    "    t_score = 0\n",
    "    ps_count = 0\n",
    "    eis_count = 0\n",
    "    sc_count = 0\n",
    "    wv_sim_score = 0\n",
    "    # key_words = [\"pore size\",\"electrolyte ion\", \"specific capacitance\"]\n",
    "    file_handler = open(file_path+\"/\"+file, \"r\", encoding=\"utf-8\")\n",
    "    text_content = file_handler.read()\n",
    "    text_content = clean_text(text_content)\n",
    "    \n",
    "    if text_content.count(\"pore size\") >= 1:\n",
    "        ps_count = text_content.count(\"pore size\")\n",
    "        t_score += 0.33\n",
    "    if text_content.count(\"electrolyte ion size\") >= 1:\n",
    "        eis_count = text_content.count(\"electrolyte ion size\")\n",
    "        t_score += 0.33\n",
    "    if text_content.count(\"specific capacitance\") >= 1:\n",
    "        sc_count = text_content.count(\"specific capacitance\")\n",
    "        t_score += 0.33\n",
    "\n",
    "    # if t_score < 0.99:\n",
    "    wv_sim_score_all = get_wv_sim(text_content,pkw_list)\n",
    "    neg_word_list = [\"never\",\"no\",\"hardly\",\"nobody\",\"none\",\"scarcely\",\"barely\",\"not\",\"no one\",\"nowhere\",\"rarely\"]\n",
    "    text_content_string = \"\"\n",
    "    text_content_string_neg = \"\"\n",
    "    sent_list = sent_tokenize(text_content)\n",
    "    for sent in sent_list:\n",
    "        if sent.count(neg_word_list[0])>=1 or  sent.count(neg_word_list[1])>=1 or sent.count(neg_word_list[2])>=1 or sent.count(neg_word_list[3])>=1 or sent.count(neg_word_list[4])>=1 or sent.count(neg_word_list[5])>=1 or sent.count(neg_word_list[6])>=1 or sent.count(neg_word_list[7])>=1 or sent.count(neg_word_list[8])>=1 or sent.count(neg_word_list[9])>=1 or sent.count(neg_word_list[10])>=1:\n",
    "            text_content_string_neg += sent\n",
    "        else:\n",
    "            text_content_string += sent\n",
    "    wv_sim_score_pos = get_wv_sim(text_content_string,pkw_list)\n",
    "    sent_analyser_p.write(file+\"\\t\"+str(ps_count)+\"\\t\"+str(eis_count)+\"\\t\"+str(sc_count)+\"\\t\"+str(t_score)+\"\\t\"+str(wv_sim_score_all)+\"\\t\"+str(wv_sim_score_pos)+\"\\n\")\n",
    "    print(\"Processed: \",file)\n",
    "    # sent_list = sent_tokenize(text_content)\n",
    "\n",
    "\n",
    "sent_analyser_p.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sent_analyser_p = open(\"sentence_analysis_report_primary_papers_positive_sent.tsv\",\"a+\",encoding=\"utf-8\")\n",
    "sent_analyser_p.write(\"file name\"+\"\\t\"+\"ps_count\"+\"\\t\"+\"eis_count\"+\"\\t\"+\"sc_count\"+\"\\t\"+\"t_score\"+\"\\t\"+\"wv_sim_score_all_sent\"+\"\\t\"+\"wv_sim_score_positive_sent\"+\"\\n\")\n",
    "# sent_analyser_p.write(\"file name\"+\"\\t\"+\"ps_count\"+\"\\t\"+\"eis_count\"+\"\\t\"+\"sc_count\"+\"\\t\"+\"t_score\"+\"\\t\"+\"wv_sim_score\"+\"\\n\")\n",
    "for file in files_list_p:\n",
    "    t_score = 0\n",
    "    ps_count = 0\n",
    "    eis_count = 0\n",
    "    sc_count = 0\n",
    "    wv_sim_score = 0\n",
    "    # key_words = [\"pore size\",\"electrolyte ion\", \"specific capacitance\"]\n",
    "    file_handler = open(file_path+\"/\"+file, \"r\", encoding=\"utf-8\")\n",
    "    text_content = file_handler.read()\n",
    "    text_content = clean_text(text_content)\n",
    "    \n",
    "    if text_content.count(\"pore size\") >= 1:\n",
    "        ps_count = text_content.count(\"pore size\")\n",
    "        t_score += 0.33\n",
    "    if text_content.count(\"electrolyte ion size\") >= 1:\n",
    "        eis_count = text_content.count(\"electrolyte ion size\")\n",
    "        t_score += 0.33\n",
    "    if text_content.count(\"specific capacitance\") >= 1:\n",
    "        sc_count = text_content.count(\"specific capacitance\")\n",
    "        t_score += 0.33\n",
    "\n",
    "    # if t_score < 0.99:\n",
    "    wv_sim_score_all = get_wv_sim(text_content,pkw_list)\n",
    "    neg_word_list = [\"never\",\"no\",\"hardly\",\"nobody\",\"none\",\"scarcely\",\"barely\",\"not\",\"no one\",\"nowhere\",\"rarely\"]\n",
    "    text_content_string = \"\"\n",
    "    text_content_string_neg = \"\"\n",
    "    sent_list = sent_tokenize(text_content)\n",
    "    for sent in sent_list:\n",
    "        if sent.count(neg_word_list[0])>=1 or  sent.count(neg_word_list[1])>=1 or sent.count(neg_word_list[2])>=1 or sent.count(neg_word_list[3])>=1 or sent.count(neg_word_list[4])>=1 or sent.count(neg_word_list[5])>=1 or sent.count(neg_word_list[6])>=1 or sent.count(neg_word_list[7])>=1 or sent.count(neg_word_list[8])>=1 or sent.count(neg_word_list[9])>=1 or sent.count(neg_word_list[10])>=1:\n",
    "            text_content_string_neg += sent\n",
    "        else:\n",
    "            text_content_string += sent\n",
    "    wv_sim_score_pos = get_wv_sim(text_content_string,pkw_list)\n",
    "    sent_analyser_p.write(file+\"\\t\"+str(ps_count)+\"\\t\"+str(eis_count)+\"\\t\"+str(sc_count)+\"\\t\"+str(t_score)+\"\\t\"+str(wv_sim_score_all)+\"\\t\"+str(wv_sim_score_pos)+\"\\n\")\n",
    "    print(\"Processed: \",file)\n",
    "    # sent_list = sent_tokenize(text_content)\n",
    "\n",
    "\n",
    "sent_analyser_p.close()\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "\n",
    "text = \"\"\"\n",
    "Zürich has a famous website https://www.zuerich.com/ \n",
    "WHICH ACCEPTS 40,000 € and adding a random string, :\n",
    "abc123def456ghi789zero0 for this demo. Also remove punctions ,. \n",
    "my phone number is 9876543210 and mail me at satkr7@gmail.com.' \n",
    "     \"\"\"\n",
    "\n",
    "clean_text = clean(s8, \n",
    "      fix_unicode=True, \n",
    "      to_ascii=True, \n",
    "      lower=True, \n",
    "      no_line_breaks=True,\n",
    "      no_urls=True, \n",
    "      no_numbers=True, \n",
    "      no_digits=True, \n",
    "      no_currency_symbols=True, \n",
    "      no_punct=True, \n",
    "      replace_with_punct=\"\", \n",
    "      replace_with_url=\"<URL>\", \n",
    "      replace_with_number=\"<NUMBER>\", \n",
    "      replace_with_digit=\"\", \n",
    "      replace_with_currency_symbol=\"<CUR>\",\n",
    "      lang='en')\n",
    "\n",
    "print(clean_text)\n",
    "\n",
    "# Output: zurich has a famous website <url> which accepts <number> <cur> and adding a random string abcdefghizero for this demo also remove punctions my phone number is <number> and mail me at satkrgmailcom"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}