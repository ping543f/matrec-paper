{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize \n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "from numpy.random import seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from itertools import chain\n",
    "from tensorflow.keras import Model,Input\n",
    "from tensorflow.keras.layers import LSTM,Embedding,Dense\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D,Bidirectional\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is blanked intentionally to write comments or other stuffs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Training with deep learning LSTM model ; to use this need to uncomment all the codes of this block\n",
    "# As metrices have been removed from Keras from version 2.0, they need to be calculated manually.\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# getting the data\n",
    "file = \"EDLC_ner_dataset_V3.csv\"\n",
    "data = pd.read_csv(file, encoding = \"utf-8\")\n",
    "\n",
    "# drop the docId column, it is not needed\n",
    "data = data.drop('docId', 1)\n",
    "\n",
    "\n",
    "# print(data.head())\n",
    "\n",
    "words = list(set(data[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "num_words = len(words)\n",
    "\n",
    "# print(f\"Total number of unique words in dataset: {num_words}\")\n",
    "\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "num_tags = len(tags)\n",
    "num_tags\n",
    "# print(\"List of tags: \" + ', '.join([tag for tag in tags]))\n",
    "# print(f\"Total Number of tags {num_tags}\")\n",
    "\n",
    "class Get_sentence(object):\n",
    "    def __init__(self,data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        agg_func = lambda s:[(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                    s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"SentenceID\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "        \n",
    "getter = Get_sentence(data)\n",
    "sentence = getter.sentences\n",
    "\n",
    "\n",
    "word_idx = {w : i + 1 for i ,w in enumerate(words)}\n",
    "tag_idx =  {t : i for i ,t in enumerate(tags)}\n",
    "\n",
    "print(\"Word idx: \",word_idx)\n",
    "# print(tag_idx)\n",
    "\n",
    "max_len = 90\n",
    "X = [[word_idx[w[0]] for w in s] for s in sentence]\n",
    "X = pad_sequences(maxlen = max_len, sequences = X, padding = 'post', value = num_words - 1)\n",
    "\n",
    "y = [[tag_idx[w[1]] for w in s] for s in sentence]\n",
    "y = pad_sequences(maxlen = max_len, sequences = y, padding = 'post', value = tag_idx['O'])\n",
    "y = [to_categorical(i, num_classes = num_tags) for i in  y]\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X, y,test_size = 0.1, random_state = 1)\n",
    "\n",
    "input_word = Input(shape = (max_len,))\n",
    "model = Embedding(input_dim = num_words, output_dim = max_len, input_length = max_len)(input_word)\n",
    "model = SpatialDropout1D(0.01)(model)\n",
    "model = Bidirectional(LSTM(units = 200,return_sequences = True, recurrent_dropout = 0.01))(model)\n",
    "out = TimeDistributed(Dense(num_tags,activation = 'softmax'))(model)\n",
    "model = Model(input_word,out)\n",
    "model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy',precision_m, recall_m, f1_m])\n",
    "\n",
    "tensorboard_cbk = TensorBoard(log_dir=\"logs/\")\n",
    "history = model.fit(x_train, np.array(y_train), batch_size = 16, verbose = 1, epochs = 1, validation_split = 0.2,callbacks=[tensorboard_cbk])\n",
    "\n",
    "\n",
    "print(\"Model Evaluation \\n===========================\\n\")\n",
    "model.evaluate(x_test, np.array(y_test))\n",
    "\n",
    "rand_sent = np.random.randint(0, x_test.shape[0]) # get a random sentence\n",
    "p = model.predict(np.array([x_test[rand_sent]]))\n",
    "p = np.argmax(p, axis = -1)\n",
    "\n",
    "y_true = np.argmax(np.array(y_test), axis = -1)[rand_sent] # get actual tags for random sentense\n",
    "\n",
    "print(\"{:20}{:20}\\t{}\\n\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for (w, t, pred) in zip(x_test[rand_sent], y_true, p[0]):\n",
    "    print(\"{:20}{:20}\\t{}\".format(words[w - 1], tags[t], tags[pred]))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "## Plotting all the metrices in a single figure\n",
    "pd.DataFrame(history.history).plot(figsize=(16,10))\n",
    "plt.show()\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################################\n",
    "#                                        Saving and loading model                                         #\n",
    "###########################################################################################################\n",
    "\n",
    "### As model can not be restored with custom metrices, hence only accuracy and loss is there and compiled the model again\n",
    "### Then fit the model and save the model\n",
    "\n",
    "# model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "# model.fit(x_train, np.array(y_train), batch_size = 16, verbose = 1, epochs = 3, validation_split = 0.2)\n",
    "\n",
    "# model.evaluate(x_test, np.array(y_test))\n",
    "\n",
    "\n",
    "# ## Save model\n",
    "# model.save('matrec.h5')\n",
    "\n",
    "\n",
    "###########################################################################################################\n",
    "#                                        Restoring a Saved model                                          #\n",
    "###########################################################################################################\n",
    "\n",
    "## restoring model and checking model\n",
    "## Currently model restoration feature is off. to load the model just uncomment the following lines and provide saved model name\n",
    "\n",
    "\n",
    "# new_model = keras.models.load_model('matrec.h5')\n",
    "# print(\"Loaded Model Evaluation \\n===========================\\n\")\n",
    "# new_model.evaluate(x_test, np.array(y_test))\n",
    "\n",
    "\n",
    "############## Model saving and resoration ends! ################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"(\\[[0-9].?\\])|(\\[[0-9].?.\\])|(\\[[0-9][0-9],[0-9][0-9]\\])\",\"\",text)\n",
    "    text = re.sub(r\"\\s{2,}\",\" \",text)\n",
    "    text = re.sub(r\"[A-Z]\\s[A-Z]\",\"\",text)\n",
    "    text = re.sub(r\"(?<=[A-Z].)\\s(?=[0-9])|(?<=[0-9])\\s(?=[A-Z])|(?<=[A-Z])\\s(?=[0-9])|(?<=\\d)\\s(?=\\d)\",\"\",text)\n",
    "    text = re.sub(r\"\\(\",\"\",text)\n",
    "    text = re.sub(r\"\\)\",\"\",text)\n",
    "    text = re.sub(r\"\\[\",\"\",text)\n",
    "    text = re.sub(r\"\\]\",\"\",text)\n",
    "    text = re.sub(r\",\",\"\",text)\n",
    "    text = re.sub(r\";\",\"\",text)\n",
    "    text = re.sub(r\"~\",\"\",text)\n",
    "    text = re.sub(r\"'\",\"\",text)\n",
    "    text = re.sub(r\"\\sÀ\",\"-\",text)\n",
    "    text = re.sub(r\"À\",\"-\",text)\n",
    "    text = re.sub(r\"Á\",\"ση\",text)\n",
    "    text = re.sub(r\"•\",\"°\",text)\n",
    "    text = re.sub(r'[‘’“”…]', '', text)\n",
    "    text = re.sub(r\"Figure\", '', text)\n",
    "    text = re.sub(r\"figure\", '', text)\n",
    "    text = re.sub(r\"FIGURE\", '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction method for any text input, will be used for each sentence after the cleaning method is applied\n",
    "# This is for deep learning model\n",
    "\n",
    "def create_test_input_from_text(text):\n",
    "    text = clean_text(text)\n",
    "    text = re.sub(r\"\\.\",\"\",text)\n",
    "    text = re.sub(r\"\\/\",\"\",text)\n",
    "    word_list = word_tokenize(text)\n",
    "#     word_list = text.split(\" \")\n",
    "#     word_list = pad_sequences(maxlen = max_len, sequences = word_list, padding = 'post', value = num_words - 1)\n",
    "    x_new = []\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            x_new.append(word_idx[word])\n",
    "        except:\n",
    "            print(\"\")\n",
    "        \n",
    "    p = model.predict(np.array([x_new]))\n",
    "    p = np.argmax(p, axis = -1)\n",
    "    print(\"{:20}\\t{}\\n\".format(\"Word\", \"Prediction\"))\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    for (w, pred) in zip(range(len(x_new)), p[0]):\n",
    "        print(\"{:20}\\t{}\".format(word_list[w], tags[pred]))\n",
    "    \n",
    "    print(word_list,'\\n=====\\n',x_new)\n",
    "\n",
    "# test_inputs = \"Lithium bistrifluoromethane sulfonyl imide LiTFSI salt are potentially a good alternative to LiPF6 since it could both improve the chemical and thermal stability as salt for electrolyte\"\n",
    "# create_test_input_from_text(test_inputs)\n",
    "\n",
    "# test_inputs = \"Li metal foil as the counter electrode and 1 M LiPF6 solution with ethylene carbonate-dimethyl carbonate as the electrolyte\"\n",
    "# create_test_input_from_text(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_wv_sim(doc1,doc2):\n",
    "    doc1 = nlp(doc1)\n",
    "    doc2 = nlp(doc2)\n",
    "    # Similarity of two documents\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "\n",
    "def get_primary_kw_sim(text):\n",
    "    doc2_f = open(\"primary-kw.txt\",\"r\",encoding = \"utf-8\")\n",
    "    doc1 = text\n",
    "    doc2 = doc2_f.read()\n",
    "#     doc1 = clean_text(doc1)\n",
    "    doc2 = clean_text(doc2)\n",
    "    jaccard_sim = get_jaccard_sim(doc1,doc2)\n",
    "    cosine_sim = get_cosine_sim(doc1,doc2)\n",
    "    wv_sim = get_wv_sim(doc1,doc2)\n",
    "    doc2_f.close()\n",
    "    return jaccard_sim, cosine_sim, wv_sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path varibale defines from where the text files will be loaded\n",
    "## for current get file method, need to put only extension of file\n",
    "## then get full filename using\n",
    "## filename = path+file \n",
    "file_path = \"pred-test\"\n",
    "def get_filenames(file_extension):\n",
    "    files = []\n",
    "    for r,d, f in os.walk(file_path+\"/\"):\n",
    "        for file in f:\n",
    "            if \".\"+file_extension in file:\n",
    "                files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = get_filenames('txt')\n",
    "\n",
    "for files in files_list:\n",
    "    print (files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files_list:\n",
    "    file_handler = open(file_path+\"/\"+file, \"r\", encoding=\"utf-8\")\n",
    "    text_content = file_handler.read()\n",
    "    create_test_input_from_text(text_content)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('base': conda)",
   "name": "python365jvsc74a57bd0d7cb94d6c01e6a3d33740a84571d93aa6b2e18b83485f7657d73a09173d8cc2c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
