{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import gensim\n",
    "# from gensim.utils import simple_preprocess\n",
    "# import gensim.corpora as corpora\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger package!\n",
    "\n",
    "## Path varibale defines from where the text files will be loaded\n",
    "file_path = \"all_papers\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wv_sim(doc1,doc2):\n",
    "    doc1 = nlp(doc1)\n",
    "    doc2 = nlp(doc2)\n",
    "    # Similarity of two documents\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "def get_cosine_sim(doc1,doc2):\n",
    "    X_list = word_tokenize(doc1)  \n",
    "    Y_list = word_tokenize(doc2) \n",
    "    sw = stopwords.words('english')  \n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # remove stop words from the string \n",
    "    X_set = {w for w in X_list if not w in sw}  \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "#     print(\"similarity: \", cosine)\n",
    "    return cosine\n",
    "\n",
    "def get_jaccard_sim(doc1, doc2): \n",
    "    a = set(doc1.split()) \n",
    "    b = set(doc2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_kw_sim(text):\n",
    "    path = file_path+\"/\"\n",
    "    filename = path+text\n",
    "    doc1_f = open(filename,\"r\",encoding = \"utf-8\")\n",
    "    doc2_f = open(\"primary-kw.txt\",\"r\",encoding = \"utf-8\")\n",
    "    doc1 = doc1_f.read()\n",
    "    doc2 = doc2_f.read()\n",
    "    doc1 = clean_text(doc1)\n",
    "    doc2 = clean_text(doc2)\n",
    "    \n",
    "    jaccard_sim = get_jaccard_sim(doc1,doc2)\n",
    "    cosine_sim = get_cosine_sim(doc1,doc2)\n",
    "    wv_sim = get_wv_sim(doc1,doc2)\n",
    "    \n",
    "    primary_writer.write(file+\",\"+str(jaccard_sim)+\",\"+str(cosine_sim)+\",\"+str(wv_sim)+\"\\n\")\n",
    "    \n",
    "    doc1_f.close()\n",
    "    doc2_f.close()\n",
    "    return jaccard_sim, cosine_sim,wv_sim\n",
    "\n",
    "\n",
    "def get_secondary_kw_sim(text):\n",
    "    path = file_path+\"/\"\n",
    "    filename = path+text\n",
    "    doc1_f = open(filename,\"r\",encoding = \"utf-8\")\n",
    "    doc2_f = open(\"secondary-kw.txt\",\"r\",encoding = \"utf-8\")\n",
    "    doc1 = doc1_f.read()\n",
    "    doc2 = doc2_f.read()\n",
    "    doc1 = clean_text(doc1)\n",
    "    doc2 = clean_text(doc2)\n",
    "    \n",
    "    jaccard_sim = get_jaccard_sim(doc1,doc2)\n",
    "    cosine_sim = get_cosine_sim(doc1,doc2)\n",
    "    wv_sim = get_wv_sim(doc1,doc2)\n",
    "    \n",
    "    secondary_writer.write(file+\",\"+str(jaccard_sim)+\",\"+str(cosine_sim)+\",\"+str(wv_sim)+\"\\n\")\n",
    "    \n",
    "    doc1_f.close()\n",
    "    doc2_f.close()\n",
    "    return jaccard_sim, cosine_sim, wv_sim\n",
    "\n",
    "\n",
    "def get_similarity(text):\n",
    "    path = file_path+\"/\"\n",
    "    filename = path+text\n",
    "    doc1_f = open(filename,\"r\",encoding = \"utf-8\")\n",
    "    docp_f = open(\"primary-kw.txt\",\"r\",encoding = \"utf-8\")\n",
    "    docs_f = open(\"secondary-kw.txt\",\"r\",encoding = \"utf-8\")\n",
    "    doc1 = doc1_f.read().lower()\n",
    "    docp = docp_f.read().lower()\n",
    "    docs = docs_f.read().lower()\n",
    "    doc1 = clean_text(doc1)\n",
    "    docp = clean_text(docp)\n",
    "    docs = clean_text(docs)\n",
    "    \n",
    "    docp_ff = open(\"primary-kw.txt\",\"r\",encoding = \"utf-8\")\n",
    "    docs_ff = open(\"secondary-kw.txt\",\"r\",encoding = \"utf-8\")\n",
    "    \n",
    "    pkw_list=docp_ff.readlines()\n",
    "    pkw_list = [clean_text(x.lower().strip(\"\\n\")) for x in pkw_list ]\n",
    "    skw_list=docs_ff.readlines()\n",
    "    skw_list = [clean_text(x.lower().strip(\"\\n\")) for x in skw_list ]\n",
    "    temp_pkw_list = []\n",
    "    temp_skw_list = []\n",
    "    not_found_pkw_list, not_found_skw_list = [], []\n",
    "    \n",
    "    for item in pkw_list:\n",
    "#         item = item.strip(\"\\n\")\n",
    "#         item = item.lower()\n",
    "#         item = clean_text(item)\n",
    "#         x = doc1.count(item)   ## For counting the occurance of the keyword\n",
    "        x = doc1.find(item)\n",
    "        if x != -1:\n",
    "            temp_pkw_list.append(item)\n",
    "#             doc_log.write(text+\",\"+item+\",\"+str(x)+\",\"+\"Primary\"+\"\\n\")\n",
    "    for item in skw_list:\n",
    "#         item = item.strip(\"\\n\")\n",
    "#         item = item.lower()\n",
    "#         item = clean_text(item)\n",
    "#         x = doc1.count(item)   ## For counting the occurance of the keyword\n",
    "        x = doc1.find(item)\n",
    "        if x != -1:\n",
    "            temp_skw_list.append(item)\n",
    "#             doc_log.write(text+\",\"+item+\",\"+str(x)+\",\"+\"Secondary\"+\"\\n\")\n",
    "            \n",
    "    not_found_pkw_list = [x for x in pkw_list if x not in temp_pkw_list]\n",
    "    not_found_skw_list = [x for x in skw_list if x not in temp_skw_list]\n",
    "    \n",
    "    doc_log.write(text+\"\\t\"+str(temp_pkw_list)+\"\\t\"+str(not_found_pkw_list)+\"\\t\"+\"Primary\"+\"\\t\"+str(len(temp_pkw_list))+\"\\t\"+str(len(not_found_pkw_list))+\"\\t\"+str(len(pkw_list))+\"\\n\")\n",
    "    doc_log.write(text+\"\\t\"+str(temp_skw_list)+\"\\t\"+str(not_found_skw_list)+\"\\t\"+\"Secondary\"+\"\\t\"+str(len(temp_skw_list))+\"\\t\"+str(len(not_found_skw_list))+\"\\t\"+str(len(skw_list))+\"\\n\")\n",
    "    \n",
    "    jaccard_sim_p = get_jaccard_sim(doc1,docp)\n",
    "    cosine_sim_p = get_cosine_sim(doc1,docp)\n",
    "    wv_sim_p = get_wv_sim(doc1,docp)\n",
    "    jaccard_sim_s = get_jaccard_sim(doc1,docs)\n",
    "    cosine_sim_s = get_cosine_sim(doc1,docs)\n",
    "    wv_sim_s = get_wv_sim(doc1,docs)\n",
    "    \n",
    "    sim_writer.write(text+\",\"+str(jaccard_sim_p)+\",\"+str(jaccard_sim_s)+\",\"+str(cosine_sim_p)+\",\"+str(cosine_sim_s)+\",\"+str(wv_sim_p)+\",\"+str(wv_sim_s)+\"\\n\")\n",
    "    \n",
    "    doc1_f.close()\n",
    "    docp_f.close()\n",
    "    docs_f.close()\n",
    "    \n",
    "#     return jaccard_sim, cosine_sim, wv_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "#     text = text.lower()\n",
    "    text = re.sub(r\"(\\[[0-9].?\\])|(\\[[0-9].?.\\])|(\\[[0-9][0-9],[0-9][0-9]\\])\",\"\",text)\n",
    "    text = re.sub(r\"\\s{2,}\",\" \",text)\n",
    "    text = re.sub(r\"[A-Z]\\s[A-Z]\",\"\",text)\n",
    "    text = re.sub(r\"(?<=[A-Z].)\\s(?=[0-9])|(?<=[0-9])\\s(?=[A-Z])|(?<=[A-Z])\\s(?=[0-9])|(?<=\\d)\\s(?=\\d)\",\"\",text)\n",
    "    text = re.sub(r\"\\(\",\"\",text)\n",
    "    text = re.sub(r\"\\)\",\"\",text)\n",
    "    text = re.sub(r\"\\[\",\"\",text)\n",
    "    text = re.sub(r\"\\]\",\"\",text)\n",
    "    text = re.sub(r\",\",\"\",text)\n",
    "    text = re.sub(r\";\",\"\",text)\n",
    "    text = re.sub(r\"~\",\"\",text)\n",
    "    text = re.sub(r\"'\",\"\",text)\n",
    "    text = re.sub(r\"\\sÀ\",\"-\",text)\n",
    "    text = re.sub(r\"À\",\"-\",text)\n",
    "    text = re.sub(r\"Á\",\"ση\",text)\n",
    "    text = re.sub(r\"•\",\"°\",text)\n",
    "    text = re.sub(r'[‘’“”…]', '', text)\n",
    "    text = re.sub(r\"Figure\", '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(file_extension):\n",
    "    files = []\n",
    "    for r,d, f in os.walk(file_path+\"/\"):\n",
    "        for file in f:\n",
    "            if \".\"+file_extension in file:\n",
    "                files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:19:33.491378 : processed file:  1.tei.xml.txt \n",
      "\n",
      "03:19:35.386410 : processed file:  10.tei.xml.txt \n",
      "\n",
      "03:19:36.890379 : processed file:  3.tei.xml.txt \n",
      "\n",
      "03:19:38.541380 : processed file:  4.tei.xml.txt \n",
      "\n",
      "03:19:40.868382 : processed file:  5.tei.xml.txt \n",
      "\n",
      "03:19:42.476381 : processed file:  6.tei.xml.txt \n",
      "\n",
      "03:19:44.095409 : processed file:  7.tei.xml.txt \n",
      "\n",
      "03:19:45.491833 : processed file:  8.tei.xml.txt \n",
      "\n",
      "03:19:47.153804 : processed file:  9.tei.xml.txt \n",
      "\n",
      "03:19:48.512805 : processed file:  edlc101.tei.xml.txt \n",
      "\n",
      "03:19:49.455835 : processed file:  edlc105.tei.xml.txt \n",
      "\n",
      "03:19:50.821833 : processed file:  edlc106.tei.xml.txt \n",
      "\n",
      "03:19:52.425833 : processed file:  edlc108.tei.xml.txt \n",
      "\n",
      "03:19:54.122802 : processed file:  edlc109.tei.xml.txt \n",
      "\n",
      "03:19:55.195833 : processed file:  edlc110.tei.xml.txt \n",
      "\n",
      "03:19:56.520802 : processed file:  edlc111.tei.xml.txt \n",
      "\n",
      "03:19:58.408802 : processed file:  edlc112.tei.xml.txt \n",
      "\n",
      "03:19:59.964832 : processed file:  edlc12.tei.xml.txt \n",
      "\n",
      "03:20:01.452833 : processed file:  edlc13.tei.xml.txt \n",
      "\n",
      "03:20:03.104833 : processed file:  edlc15.tei.xml.txt \n",
      "\n",
      "03:20:04.078835 : processed file:  edlc16.tei.xml.txt \n",
      "\n",
      "03:20:05.517805 : processed file:  edlc17.tei.xml.txt \n",
      "\n",
      "03:20:06.822803 : processed file:  edlc20.tei.xml.txt \n",
      "\n",
      "03:20:09.979806 : processed file:  edlc21.tei.xml.txt \n",
      "\n",
      "03:20:10.967835 : processed file:  edlc23.tei.xml.txt \n",
      "\n",
      "03:20:12.268806 : processed file:  edlc24.tei.xml.txt \n",
      "\n",
      "03:20:13.956803 : processed file:  edlc26.tei.xml.txt \n",
      "\n",
      "03:20:15.620803 : processed file:  edlc27.tei.xml.txt \n",
      "\n",
      "03:20:16.644804 : processed file:  edlc28.tei.xml.txt \n",
      "\n",
      "03:20:17.641803 : processed file:  edlc29.tei.xml.txt \n",
      "\n",
      "03:20:18.454806 : processed file:  edlc30.tei.xml.txt \n",
      "\n",
      "03:20:20.469801 : processed file:  edlc31.tei.xml.txt \n",
      "\n",
      "03:20:21.603801 : processed file:  edlc32.tei.xml.txt \n",
      "\n",
      "03:20:23.173833 : processed file:  edlc33.tei.xml.txt \n",
      "\n",
      "03:20:24.540832 : processed file:  edlc35.tei.xml.txt \n",
      "\n",
      "03:20:27.094804 : processed file:  edlc39.tei.xml.txt \n",
      "\n",
      "03:20:28.824833 : processed file:  edlc40.tei.xml.txt \n",
      "\n",
      "03:20:30.070833 : processed file:  edlc41.tei.xml.txt \n",
      "\n",
      "03:20:31.806805 : processed file:  edlc42.tei.xml.txt \n",
      "\n",
      "03:20:33.140833 : processed file:  edlc43.tei.xml.txt \n",
      "\n",
      "03:20:34.848821 : processed file:  edlc45.tei.xml.txt \n",
      "\n",
      "03:20:36.074817 : processed file:  edlc46.tei.xml.txt \n",
      "\n",
      "03:20:37.531849 : processed file:  edlc47.tei.xml.txt \n",
      "\n",
      "03:20:38.736817 : processed file:  edlc49.tei.xml.txt \n",
      "\n",
      "03:20:40.314820 : processed file:  edlc50.tei.xml.txt \n",
      "\n",
      "03:20:42.198817 : processed file:  edlc51.tei.xml.txt \n",
      "\n",
      "03:20:43.939849 : processed file:  edlc52.tei.xml.txt \n",
      "\n",
      "03:20:45.230816 : processed file:  edlc53.tei.xml.txt \n",
      "\n",
      "03:20:46.309820 : processed file:  edlc54.tei.xml.txt \n",
      "\n",
      "03:20:47.421819 : processed file:  edlc55.tei.xml.txt \n",
      "\n",
      "03:20:48.826820 : processed file:  edlc56.tei.xml.txt \n",
      "\n",
      "03:20:50.323818 : processed file:  edlc57.tei.xml.txt \n",
      "\n",
      "03:20:51.191816 : processed file:  edlc58.tei.xml.txt \n",
      "\n",
      "03:20:52.413818 : processed file:  edlc59.tei.xml.txt \n",
      "\n",
      "03:20:53.634817 : processed file:  edlc61.tei.xml.txt \n",
      "\n",
      "03:20:54.843817 : processed file:  edlc62.tei.xml.txt \n",
      "\n",
      "03:20:55.822820 : processed file:  edlc63.tei.xml.txt \n",
      "\n",
      "03:20:57.352820 : processed file:  edlc64.tei.xml.txt \n",
      "\n",
      "03:20:58.690817 : processed file:  edlc65.tei.xml.txt \n",
      "\n",
      "03:20:59.946820 : processed file:  edlc66.tei.xml.txt \n",
      "\n",
      "03:21:01.679817 : processed file:  edlc68.tei.xml.txt \n",
      "\n",
      "03:21:03.150853 : processed file:  edlc70.tei.xml.txt \n",
      "\n",
      "03:21:04.209851 : processed file:  edlc71.tei.xml.txt \n",
      "\n",
      "03:21:05.686817 : processed file:  edlc72.tei.xml.txt \n",
      "\n",
      "03:21:07.041818 : processed file:  edlc73.tei.xml.txt \n",
      "\n",
      "03:21:09.910817 : processed file:  edlc74.tei.xml.txt \n",
      "\n",
      "03:21:12.025818 : processed file:  edlc75.tei.xml.txt \n",
      "\n",
      "03:21:13.942818 : processed file:  edlc76.tei.xml.txt \n",
      "\n",
      "03:21:15.098818 : processed file:  edlc78.tei.xml.txt \n",
      "\n",
      "03:21:16.325818 : processed file:  edlc79.tei.xml.txt \n",
      "\n",
      "03:21:17.521835 : processed file:  edlc80.tei.xml.txt \n",
      "\n",
      "03:21:18.692817 : processed file:  edlc81.tei.xml.txt \n",
      "\n",
      "03:21:20.590847 : processed file:  edlc82.tei.xml.txt \n",
      "\n",
      "03:21:21.697818 : processed file:  edlc83.tei.xml.txt \n",
      "\n",
      "03:21:26.369818 : processed file:  edlc84.tei.xml.txt \n",
      "\n",
      "03:21:30.471830 : processed file:  edlc85.tei.xml.txt \n",
      "\n",
      "03:21:31.965818 : processed file:  edlc86.tei.xml.txt \n",
      "\n",
      "03:21:33.226818 : processed file:  edlc89.tei.xml.txt \n",
      "\n",
      "03:21:34.777817 : processed file:  edlc90.tei.xml.txt \n",
      "\n",
      "03:21:36.050817 : processed file:  edlc91.tei.xml.txt \n",
      "\n",
      "03:21:36.693818 : processed file:  edlc94.tei.xml.txt \n",
      "\n",
      "03:21:37.813817 : processed file:  edlc95.tei.xml.txt \n",
      "\n",
      "03:21:39.300816 : processed file:  edlc97.tei.xml.txt \n",
      "\n",
      "Finished process at :  03:21:39.302820\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# primary_writer = open(\"ALL_primary_kw_similarity.csv\",\"a+\",encoding=\"utf-8\")\n",
    "# secondary_writer = open(\"ALL_secondary_kw_similarity.csv\",\"a+\",encoding=\"utf-8\")\n",
    "sim_writer = open(\"ALL_kw_similarity_V2.csv\",\"a+\",encoding=\"utf-8\")\n",
    "sim_writer.write(\"Filename,Jaccard_primary,Jaccard_secondary,Cosine_primary,Cosine_secondary,Wordvector_primary,Wordvector_secondary\\n\")\n",
    "doc_log = open(\"KW_list_found_not_found_V1.tsv\",\"a+\",encoding=\"utf-8\")\n",
    "doc_log.write(\"Filename \\t Found Keywords \\t Not Found Keywords \\t KW Level \\t Found Number \\t Not Found Number \\t Total Number \\n\")\n",
    "# primary_writer.write(\"Filename,Jaccard,Cosine,Wordvector\\n\")\n",
    "# secondary_writer.write(\"Filename,Jaccard,Cosine,Wordvector\\n\")\n",
    "\n",
    "\n",
    "files_=get_filenames('txt')    \n",
    "\n",
    "for file in files_:\n",
    "    get_similarity(file)\n",
    "    print(datetime.now().time(),\": processed file: \",file,\"\\n\") \n",
    "#     get_primary_kw_sim(file)\n",
    "#     get_secondary_kw_sim(file)\n",
    "    \n",
    "\n",
    "    \n",
    "# primary_writer.close()\n",
    "# secondary_writer.close()\n",
    "sim_writer.close()\n",
    "doc_log.close()\n",
    "print(\"Finished process at : \", datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
